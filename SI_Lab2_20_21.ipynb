{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SI_Lab2_20_21.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N8wgqtIpdu7"
      },
      "source": [
        "#### ![FCT](https://www.acessolivre.pt/wp-content/uploads/2015/10/Imagem-UNL.jpg \"FCT-UNL\")\n",
        "\n",
        "## **SI 20/21 - Predicting the Output for a Multistage Factory Process using Machine Learning**\n",
        "\n",
        "---\n",
        "\n",
        "### **Problem Description**\n",
        "\n",
        "A multistage manufacturing process refers to a system encompassing multiple components, stations or stages required to finish the final product, being very common in modern manufacturing. Typically, the quality of the final product depends on the complex interactions between different stages. Thus, the quality characteristics at one stage are not only influenced by local variations at that stage, but also by variations propagated from upstream stages.\n",
        "\n",
        "In this context predictive models based on machine learning can be used in the development of real time process controllers, anomaly detection, quality control, etc.\n",
        "\n",
        "<img width=400px align=\"right\" src=\"https://p2.piqsels.com/preview/730/1011/600/plant-production-industry-manufactures-thumbnail.jpg\" title=\"Source: [Online] https://p2.piqsels.com/preview/730/1011/600/plant-production-industry-manufactures-thumbnail.jpg\">\n",
        "      \n",
        "### **Data**\n",
        "\n",
        "The dataset contains data sampled every minute from one production run of a multistage continuous flow manufacturing process spanning several hours. The focus is put on three machines operating in parallel at the first stage, which then feed their outputs into a step that combines the flows. \n",
        "\n",
        "The output from the combiner step is measured in 6 different locations. The **goal** is to predict the measurements of the output from this stage, based on the data from the shopfloor environment and operations upstream. \n",
        "\n",
        "### **Required Material**\n",
        "\n",
        "The assignment will be developed using **Google Colaboratory** to ensure that everyone has access to the same computational resources, without requiring additional configuration effort.\n",
        "\n",
        "https://colab.research.google.com/\n",
        "\n",
        "Google Colab provides a platform that allows you to write and execute Python notebooks\n",
        "in the browser, with minimal configuration required and free access to GPUs.\n",
        "\n",
        "The .ipynb provided by the the teaching staff should be uploaded to the student's Google Drive (using the University's student account) and opened directly in Colab.\n",
        "\n",
        "**Recommended Packages:** *Pandas* and *Numpy* (data structures and manipulation), *Scikit-Learn* (Machine Learning), *Matplotlib* and *Seaborn* (visualization).\n",
        "\n",
        "### **Submission Guidelines and Deadline**\n",
        "* Completed projects should be submitted via the course's **Moodle** page before the end of the deadline.\n",
        "* Projects should be executed in groups of 2 or 3 (maximum) students. \n",
        "* The project should be submitted as a **single .ipynb** notebook file, named following the template **\"*studentNumber1_studentNumber2_studentNumber3.rar*\"** (e.g. *31444_31445_31446.rar*) containing:\n",
        "   * The complete jupyter notebook contemplating the data analysis / machine learning part of the assignment. You can use the template provided in the CLIP platform which simultaneous serves as a guideline, project template and report.\n",
        "* Deadline is **19 of December, 23:59 GMT**.\n",
        "\n",
        "### **Evaluation Criteria**\n",
        "\n",
        "All of the goal/value pairs listed below are based on the assumption that a correct implementation is submitted. \n",
        "\n",
        "Feel free to fill in the *Completed* column in accordance to your submission for the discussion (replace \"*-*\" with \"*X*\" when suitable).\n",
        "\n",
        "| Goal                                                     | Value | Completed |\n",
        "|:--------------------------------------------------------|:------|:----------:| \n",
        "| üíæ Loading and preparing train/test data                | 4     |     -      |\n",
        "| üèãÔ∏è Training **at least 3** different regressors         | 6     |     -      |\n",
        "| üíØ Evaluating each regressor using adequate metrics     | 4     |     -      |\n",
        "| üìà Plotting the results for comparison                  | 3     |     -      |\n",
        "| üí¨ Discussing the results                               | 1     |     -      |\n",
        "| ‚ùì  Additional features (Free choice)                           | 2     |    -       |\n",
        "\n",
        "Please refer to the lab staff for additional info regarding possible additional features. Examples include for instance performing feature extraction on the original dataset to check if newly created features improve performance or tuning the models' hyper-parameters (please refer to the [documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) which includes several examples) to optimize performance.\n",
        "\n",
        "### **Lab Planning**\n",
        "\n",
        "* Lab 1 (week of 30/11/20) - Intro, Data Ingestion and Exploration\n",
        "* Lab 2 (week of 07/12/20) - Model Training and Evaluation\n",
        "* Lab 3 (week of 14/12/20) - Visualization and Bonus Features\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdHclDWYhavd"
      },
      "source": [
        "### **TO DO**: \n",
        "1) Load the training data \n",
        "* Use read_csv (pandas) to load data into a dataframe \n",
        "* Use the dataframe's head() to check the first few rows \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-eqwynhJkIj"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9XoTOh3hyjR"
      },
      "source": [
        "### **TO DO**: \n",
        "2) Check the shape and sum of missing values per feature using the .shape and .isna().sum() functions from Pandas' Dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIz86mRrKBxt"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtbRHMnLh2Ei"
      },
      "source": [
        "### **TO DO**: \n",
        "3) Drop rows containing missing values using .dropna . Verify if rows were correctly dropped by printing the dataframe's shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lsTGGH-KIH8"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8oUrzuMh3XQ"
      },
      "source": [
        "### **TO DO**: \n",
        "4) Generate descriptive statistics. These include those that summarize the central tendency, dispersion and shape of a dataset‚Äôs distribution, excluding NaN values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGxqsLnIKPNs"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmLDR0BGh4V6"
      },
      "source": [
        "### **TO DO**: \n",
        "5) Plot the correlation matrix for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCeL0lbkKTGL"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3F1BdbLh5dn"
      },
      "source": [
        "### **TO DO**: \n",
        "6) Build a dictionary with key/value pairs for each metric and calculate its value using the corresponding sklearn.metrics call. Build a pandas dataframe from the dict using pd.DataFrame.from_dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtgC3Tc_KjCv"
      },
      "source": [
        "#TBD add the necessary imports\n",
        "\n",
        "def evaluate_regressor(name, y_test, y_pred):  \n",
        "    \"\"\"Calculate the main regression metrics supporting multi-target cases for a given model.  \n",
        "\n",
        "    Args:\n",
        "        name (str): The trained model's name\n",
        "        y_test (series): Contains the ground truth values (aka y_true)\n",
        "        y_pred (series): Contains the predicted values for the test set\n",
        "        \n",
        "    Returns:\n",
        "        df_metrics (DataFrame): The predicted metrics in a DataFrame\n",
        "        \n",
        "    \"\"\"\n",
        "    dict_metrics = {\n",
        "        'Explained Variance': #TBD\n",
        "        'MAE': #TBD\n",
        "        'MSE': #TBD\n",
        "        'RMSE': #TBD\n",
        "        'R2': #TBD\n",
        "    }\n",
        "    df_metrics = #TBD\n",
        "    df_metrics.columns = [name]\n",
        "    return df_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jekSMaath6Jq"
      },
      "source": [
        "### **TO DO**: \n",
        "7) Train/Test Split:\n",
        "* Split the dataset into X (inputs) and Y (outputs) (suggestion: use .iloc)\n",
        "\n",
        "* Then use train_test_split from scikit-learn to further split them into X_train, X_test, y_train, and y_test with and appropriate test size (e.g. 20% or 33%)\n",
        "\n",
        "* Ensure the data is shuffled, since we are not interested in the sequence, but instead want to see if there's any relation between the machine/process parameters in the multistage process upstream and the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4KQQPFjKjE4"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJGNld6Uh6zl"
      },
      "source": [
        "### **TO DO**: \n",
        "8) Store the time_stamp for later in case we want to do some visualizations, and remove it from training/test set with .drop.\n",
        "\n",
        "As previously stated, we are not interested in the temporal dimension of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfxk2qosLcLU"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFisQT29h7b9"
      },
      "source": [
        "### **TO DO**: \n",
        "\n",
        "9) Train at least 3 different regression models. Experiment with the model parameters (see the scikit-learn documentation) to aim for the best performance you can achieve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l01sqzOKjJh"
      },
      "source": [
        "# Imports\n",
        "\n",
        "# Instantiate regressor\n",
        "\n",
        "# Fit the regressor to the training data\n",
        "\n",
        "# Generate predictions from test set\n",
        "\n",
        "# Evaluate using evaluate_regressor\n",
        "df_rfr_metrics = #TBD\n",
        "df_rfr_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wTl-EFNmk7K"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nvLTdj2mldV"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8G0VGl7h8ET"
      },
      "source": [
        "### **TO DO**: \n",
        "10) Implement a function that prints the RMSE and R2 values per target for a given model. Then call it for each of your models to get an overview of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LduAjQm4Ve3W"
      },
      "source": [
        "def print_rmse_per_target(model_name, y_test, y_pred):\n",
        "    #TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSH_ScBsMejW"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FVZ7B6soJ0k"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfKZT3lYozPU"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWOI65e6iB58"
      },
      "source": [
        "### **TO DO**: \n",
        "11) Plot the True Values VS Predictions for each of the output measurements for your best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in-mnTj5-cWa"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL5avkfxaRc0"
      },
      "source": [
        "##Extras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUbYNnJcaVcZ"
      },
      "source": [
        "#TBD"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}